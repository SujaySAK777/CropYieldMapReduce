# ğŸŒ¾ CropYieldMapReduce â€” Big Data Crop Analysis & Prediction

**CropYieldMapReduce** is a practical mini-project that demonstrates Big Data processing in agriculture using:

- ğŸ“ˆ **Apache Hadoop MapReduce** for analyzing crop land utilization metrics.
- âš¡ **Apache Spark MLlib** for predicting average crop yield based on input factors.
- ğŸ—‚ï¸ **Streamlit Web App** for an interactive user interface.
- ğŸ”¥ **System Monitoring** to observe CPU, Memory, Disk usage & approximate heat generation while running heavy tasks.

This project is useful for understanding Big Data pipelines, parallel processing, and how real-world datasets can be handled and visualized efficiently.

---

## ğŸ§© Key Highlights

âœ… Data visualization for crop & land utilization\
âœ… MLlib-based crop yield prediction\
âœ… Hadoop MapReduce with CPU/Memory/Temperature monitoring\
âœ… One-click web app built with Streamlit\
âœ… Simple to clone & run on Windows

---

## âš™ï¸ Software Requirements

Before you begin, make sure you have installed:

- **Python** (3.10 or higher) â€” for Streamlit, pandas, and scripts.
- **Java JDK** (version 8 or higher) â€” required by Hadoop and Spark.
- **Hadoop** (3.x) â€” for running MapReduce tasks.
- **Apache Spark** (3.5.5 recommended) â€” for MLlib-based predictions.
- **Streamlit** â€” to run the web app interface.
- **psutil** â€” Python library for monitoring system resources.
- **pywin32** and **wmi** â€” for reading CPU temperature through OpenHardwareMonitor.
- **OpenHardwareMonitor** â€” must be running to expose CPU temperature sensors via WMI.

---

## ğŸ“¥ **How to Clone the Repository**

```bash
git clone https://github.com/SujaySAK777/CropYieldMapReduce.git
cd CropYieldMapReduce
```

---

## ğŸ **Python Environment Setup**

**1ï¸âƒ£ Create a Virtual Environment**

```bash
python -m venv venv
```

**2ï¸âƒ£ Activate the Environment**

```bash
# Windows
venv\Scripts\activate
```

**3ï¸âƒ£ Install Python Dependencies**

First, create a `requirements.txt` file with:

```
streamlit
pandas
plotly
scikit-learn
psutil
pyspark
pywin32
wmi
```

Then install all packages:

```bash
pip install -r requirements.txt
```

---

## ğŸ—„ï¸ **Hadoop Setup**

1ï¸âƒ£ Download Hadoop (prebuilt binaries) and extract it.\
2ï¸âƒ£ Configure `HADOOP_HOME` and `JAVA_HOME` in your environment variables.\
3ï¸âƒ£ Format the NameNode:

```bash
hdfs namenode -format
```

4ï¸âƒ£ Start Hadoop services:

```bash
start-dfs.cmd
start-yarn.cmd
```

5ï¸âƒ£ Place your input data into HDFS:

```bash
hdfs dfs -mkdir -p /user/hadoop/input
hdfs dfs -put cropdata.csv /user/hadoop/input/
```

---

## âš¡ **Spark Setup**

1ï¸âƒ£ Download Spark with Hadoop bundled.\
2ï¸âƒ£ Add `SPARK_HOME` to your PATH or run `spark-submit` using its full path.\
3ï¸âƒ£ Confirm itâ€™s working:

```bash
spark-submit --version
```

---

## ğŸ”¥ **Heat & Resource Monitoring Setup**

- Download and run **OpenHardwareMonitor** on your machine.
- Launch `OpenHardwareMonitor.exe` **before** running the monitoring script to expose hardware sensors.
- The Python monitoring script (`monitor_hadoop.py`) uses `psutil` for CPU, RAM, Disk and `wmi` to get CPU temperature via OpenHardwareMonitorâ€™s WMI namespace.

---

## ğŸš€ **How to Run**

### âœ… 1ï¸âƒ£ **Launch the Streamlit Web App**

Run:

```bash
streamlit run main.py
```

Your browser will open with 3 sections:

**a) Spark MLlib Prediction**

- Select State & Crop (loaded dynamically from your dataset).
- Input values: area, agri land, barren land, rainfall, fertilizer, pesticide.
- Click **Predict** â†’ This runs `spark_easy_web.py` using `spark-submit` and shows the predicted yield.

**b) Visualization Dashboard**

- Explore plots and trends from `crop_analysis_output.csv`.
- See state-wise land use, barren land %, yield trends, and more.

**c) Hadoop Resource Monitoring**

- Click **Run MapReduce & Monitor**.
- Runs the Hadoop Streaming job (`mapper.py` + `reducer.py`).
- Simultaneously logs CPU %, Memory %, Disk usage %, and CPU Temp (if OpenHardwareMonitor is running).
- Shows 4 separate time-series charts for system usage.

---

## ğŸ”§ **How Heat Monitoring Works**

- The `monitor_hadoop.py` script starts the MapReduce job.
- Uses `psutil` for CPU, RAM, Disk.
- Uses `wmi` to read CPU temperature from OpenHardwareMonitor.
- Logs all metrics every second to `real_system_usage.csv`.
- The Streamlit dashboard displays graphs for each metric under **Hadoop Resource Monitoring**.

---

## ğŸ—ƒï¸ **How MapReduce Works**

- **Mapper:** Reads each row, extracts needed columns: State, Crop, Area, Yield, Agri Land, Barren Land, Rainfall, Fertilizer, Pesticide.
- **Reducer:** Groups by (State, Crop) â†’ computes average values per group.
- **Output:** Shows average agri land, barren land, yield, rainfall, and fertilizer/pesticide use per hectare.

---

## ğŸ’¡ **Key Benefits**

âœ… Learn how big data splits and parallelizes tasks with Hadoop MapReduce.\
âœ… See how Sparkâ€™s MLlib can learn from past data to predict future yields.\
âœ… Visualize the resource footprint of big data tasks â€” CPU, memory, disk, and approximate heat.\
âœ… No complex manual commands â€” run everything through a single Streamlit app.

---

## âš ï¸ **Important Notes**

- Always launch **OpenHardwareMonitor** first to make CPU temperature available.
- If your `spark-submit` is not in PATH, update `main.py` with the full absolute path to `spark-submit.cmd`.
- Make sure your CSV (`crop_analysis_output.csv`) covers all States & Crops you want to select.

---

## ğŸ‰ **Done!**

âœ… Youâ€™re ready to clone, run, explore, and learn from your very own Big Data mini-project.\
Open an issue or pull request on your GitHub if you improve it!

